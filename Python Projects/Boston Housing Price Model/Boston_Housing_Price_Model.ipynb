{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_HZblvdCRka"
      },
      "source": [
        "<img src=https://i.imgur.com/WKQ0nH2.jpg height=350>\n",
        "\n",
        "# Setup and Context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAGWVzc2Cdu7"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Welcome to Boston Massachusetts in the 1970s! This project aims to build a model that can provide a price estimate based on a home's characteristics like:\n",
        "* The number of rooms\n",
        "* The distance to employment centres\n",
        "* Local area income levels\n",
        "* Student to teacher ratios in local schools etc\n",
        "\n",
        "<img src=https://i.imgur.com/WfUSSP7.png height=350>\n",
        "\n",
        "This Project's Objectives are as follows:\n",
        "\n",
        "1. Analyse and explore the Boston house price data\n",
        "2. Split data for training and testing\n",
        "3. Run a Multivariable Regression\n",
        "4. Evaluate how the model's coefficients and residuals\n",
        "5. Transform data to improve your model performance\n",
        "6. Use model to estimate a property price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAiLrvQiCs5h"
      },
      "source": [
        "### Upgrade plotly (only Google Colab Notebook)\n",
        "\n",
        "Google Colab may not be running the latest version of plotly. If you're working in Google Colab, uncomment the line below, run the cell, and restart your notebook server. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5G1mC4dCmcI"
      },
      "source": [
        "# %pip install --upgrade plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg7IDCTd-d2h"
      },
      "source": [
        "###  Import Statements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iYmJ3Fb-d2i"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YexUjiVdC0Oe"
      },
      "source": [
        "### Notebook Presentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUvdOhPIC4Me"
      },
      "source": [
        "pd.options.display.float_format = '{:,.2f}'.format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRlvO4zw-d2l"
      },
      "source": [
        "# Load the Data\n",
        "\n",
        "The first column in the .csv file just has the row numbers, so it will be used as the index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlG_B81bYakP"
      },
      "source": [
        "housing_file = './boston.csv'\n",
        "data = pd.read_csv(housing_file, index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKyJsSdEChd-"
      },
      "source": [
        "### Understand the Boston House Price Dataset\n",
        "\n",
        "---------------------------\n",
        "\n",
        "**Characteristics:**  \n",
        "\n",
        "    :Number of Instances: 506 \n",
        "\n",
        "    :Number of Attributes: 13 numeric/categorical predictive. The Median Value (attribute 14) is the target.\n",
        "\n",
        "    :Attribute Information (in order):\n",
        "        1. CRIM     per capita crime rate by town\n",
        "        2. ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "        3. INDUS    proportion of non-retail business acres per town\n",
        "        4. CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "        5. NOX      nitric oxides concentration (parts per 10 million)\n",
        "        6. RM       average number of rooms per dwelling\n",
        "        7. AGE      proportion of owner-occupied units built prior to 1940\n",
        "        8. DIS      weighted distances to five Boston employment centres\n",
        "        9. RAD      index of accessibility to radial highways\n",
        "        10. TAX      full-value property-tax rate per $10,000\n",
        "        11. PTRATIO  pupil-teacher ratio by town\n",
        "        12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "        13. LSTAT    % lower status of the population\n",
        "        14. PRICE     Median value of owner-occupied homes in $1000's\n",
        "        \n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
        "\n",
        "This is a copy of [UCI ML housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/). This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. You can find the [original research paper here](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf?sequence=1&isAllowed=y). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTICpcuYD6BP"
      },
      "source": [
        "# Preliminary Data Exploration 🔎\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whPNpmlF86sh"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0hzZhBA-d2_"
      },
      "source": [
        "## Data Cleaning - Check for Missing Values and Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dkhmpuy86pV"
      },
      "source": [
        "data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().any()"
      ],
      "metadata": {
        "id": "zODx811McA3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZvNWb0EGsuP"
      },
      "source": [
        "## Descriptive Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmf-vAdK8_he"
      },
      "source": [
        "print(f'Average student to teacher ratio: {data.PTRATIO.mean():.1f}')\n",
        "print(f'Average home price in: ${data.PRICE.mean()*1000:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHAS is a boolean indicating whether or not a house is near the Charles River."
      ],
      "metadata": {
        "id": "VydKsEzrt8yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.CHAS.describe().loc[['min', 'max']]"
      ],
      "metadata": {
        "id": "H4rl-3Dst0Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the mean, minimum and maximum values for rooms per dwelling (RM) in the dataset:"
      ],
      "metadata": {
        "id": "Hh8FIQN5tkFk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJOkC5hI8_fF"
      },
      "source": [
        "data.RM.describe().loc[['mean','min','max']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "v5U4hAo_-d3D"
      },
      "source": [
        "## Visualizing the Features\n",
        "\n",
        "Having looked at some descriptive statistics, we will use Seaborn to create a bar chart and superimpose the Kernel Density Estimate (KDE) for the following variables: \n",
        "* PRICE: The home price in thousands.\n",
        "* RM: the average number of rooms per owner unit.\n",
        "* DIS: the weighted distance to the 5 Boston employment centres i.e., the estimated length of the commute.\n",
        "* RAD: the index of accessibility to highways. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRGb4b7bkQbe"
      },
      "source": [
        "#### House Prices 💰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A85hcxas9BhW"
      },
      "source": [
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(data=data,\n",
        "              x='PRICE',\n",
        "              kde=True,\n",
        "              aspect=2)\n",
        "  \n",
        "  plt.title('1970s Boston Housing Price Distribution')\n",
        "  plt.xlabel('Price in $1000s')\n",
        "  plt.ylabel('Number of Homes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the spike in homes priced at the tailend of ~$50k."
      ],
      "metadata": {
        "id": "hyF1llZci-a9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mqTPLQMlxxz"
      },
      "source": [
        "#### Distance to Employment - Length of Commute 🚗"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U38tDs119DBe"
      },
      "source": [
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(x=data.DIS,\n",
        "              kde=True,\n",
        "              aspect=2,)\n",
        "  \n",
        "  plt.title('1970s Estimated Commute Time Distribution for Boston House')\n",
        "  plt.xlabel('Estimated Commute Time')\n",
        "  plt.ylabel('Number of Houses')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The average commute time is {data.DIS.mean():.2} minutes.')"
      ],
      "metadata": {
        "id": "7ehEuYdTjJz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nMMsL9DkUl9"
      },
      "source": [
        "#### Number of Rooms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfmHNdzL9Ef-"
      },
      "source": [
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(x=data.RM,\n",
        "              kde=True,\n",
        "              aspect=2,)\n",
        "  \n",
        "  plt.title('1970s Average Number of Rooms per Boston House')\n",
        "  plt.xlabel('Average Number of Rooms')\n",
        "  plt.ylabel('Number of Houses')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is very close to normal with an average room count of:"
      ],
      "metadata": {
        "id": "zS6NUsyXjidH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.RM.mean()"
      ],
      "metadata": {
        "id": "24EM-FEjjhST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sz47c2UkXXw"
      },
      "source": [
        "#### Access to Highways 🛣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKpT7wD39HGU"
      },
      "source": [
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(x=data.RAD,\n",
        "              kde=True,\n",
        "              aspect=2,)\n",
        "  \n",
        "  plt.title('1970s Boston House Highway Accessibility')\n",
        "  plt.xlabel('Highway Accessibility')\n",
        "  plt.ylabel('Number of Houses')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is highly irregular with a large gap for highway accessibility."
      ],
      "metadata": {
        "id": "oYfti6b7jpt9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxep6lvqkbwe"
      },
      "source": [
        "#### Next to the River? ⛵️\n",
        "\n",
        "Now let's use plotly to visualize CHAS, the proportion of dwellings with access to the Charles River."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "river_access = data.CHAS.value_counts()\n",
        "river_access"
      ],
      "metadata": {
        "id": "fuLrSOFnklju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2244H8hx9NV_"
      },
      "source": [
        "bar = px.bar(x=['No', 'Yes'],\n",
        "             y=river_access.values,\n",
        "             color=river_access.values,\n",
        "             color_continuous_scale=px.colors.sequential.haline,\n",
        "             title=\"Next to Charles River?\")\n",
        "\n",
        "bar.update_layout(xaxis_title='Propertly Located Next to River?',\n",
        "                  yaxis_title='Number of Houses',\n",
        "                  coloraxis_showscale=False)\n",
        "\n",
        "\n",
        "bar.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0lBoTqtu01d"
      },
      "source": [
        "bar = px.pie(names=['No', 'Yes'],\n",
        "             values=river_access.values,\n",
        "             title=\"Next to Charles River?\")\n",
        "\n",
        "bar.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPuSz98jsmIz"
      },
      "source": [
        "<img src=https://i.imgur.com/b5UaBal.jpg height=350>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_M1pqzVUas7"
      },
      "source": [
        "# Understand the Relationships in the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbMSfXfOqA5R"
      },
      "source": [
        "### Run a Pair Plot\n",
        "\n",
        "Next, let's run a [Seaborn `.pairplot()`](https://seaborn.pydata.org/generated/seaborn.pairplot.html?highlight=pairplot#seaborn.pairplot) to visualise all the relationships within the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data, kind='reg', plot_kws={'line_kws':{'color': 'cyan'}})"
      ],
      "metadata": {
        "id": "IqvZz7i2_JEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roOODvUVu6pk"
      },
      "source": [
        "Now lets use [Seaborn's `.jointplot()`](https://seaborn.pydata.org/generated/seaborn.jointplot.html) to look at some of the relationships in more detail. Specifically:\n",
        "\n",
        "* DIS and NOX\n",
        "* INDUS vs NOX\n",
        "* LSTAT vs RM\n",
        "* LSTAT vs PRICE\n",
        "* RM vs PRICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OQZTKQOma5A"
      },
      "source": [
        "#### Distance from Employment vs. Pollution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the jointplot below, pollution has a negative correlation with distance from employment."
      ],
      "metadata": {
        "id": "1apD0yok5hKC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dToSDGBU9ThU"
      },
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.jointplot(x=data.DIS,\n",
        "                y=data.NOX,\n",
        "                kind='scatter',\n",
        "                color='green',\n",
        "                joint_kws={'alpha': 0.7})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKc6vwxCsbxz"
      },
      "source": [
        "#### Proportion of Non-Retail Industry 🏭🏭🏭 versus Pollution \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsurprisingly, pollution levels have a positive correlation with more industries, but the data has more spread compared to the last jointplot."
      ],
      "metadata": {
        "id": "mFvOpKGJ9LmX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvmo-IDC9VaM"
      },
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.jointplot(x=data.INDUS,\n",
        "                y=data.NOX,\n",
        "                kind='scatter',\n",
        "                color='orange',\n",
        "                joint_kws={'alpha': 0.7})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssO_ouF2r8bj"
      },
      "source": [
        "#### % of Lower Income Population vs Average Number of Rooms\n",
        "\n",
        "The correlation between LSTAT and RM is a bit less clear than the last two jointplots, but it can be seen that lower LSTAT's have higher RM and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP9nLYo69W3E"
      },
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.jointplot(x=data.LSTAT,\n",
        "                y=data.RM,\n",
        "                kind='scatter',\n",
        "                color='crimson',\n",
        "                joint_kws={'alpha': 0.7})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs-5lByYr3tH"
      },
      "source": [
        "#### % of Lower Income Population versus Home Price\n",
        "\n",
        "According to the figure below, home prices are higher as the population of lower income in an area is smaller. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzDPv2mA9ZLS"
      },
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.jointplot(x=data.LSTAT,\n",
        "                y=data.PRICE,\n",
        "                kind='scatter',\n",
        "                joint_kws={'alpha': 0.7})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qMWIrs9sIGn"
      },
      "source": [
        "#### Number of Rooms versus Home Value\n",
        "\n",
        "**Challenge** \n",
        "\n",
        "Higher number of rooms have a clear positive correlation with home value. 😊 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKiSjKor9a7l"
      },
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.jointplot(x=data.RM,\n",
        "                y=data.PRICE,\n",
        "                kind='scatter',\n",
        "                color='darkblue',\n",
        "                joint_kws={'alpha': 0.7})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QBQWGOph-d36"
      },
      "source": [
        "# Split Training & Test Dataset\n",
        "\n",
        "We *can't* use all 506 entries in our dataset to train our model. The reason is that we want to evaluate our model on data that it hasn't seen yet (i.e., out-of-sample data). That way we can get a better idea of its performance in the real world. So to do so we'll:\n",
        "\n",
        "* Import the [`train_test_split()` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn\n",
        "* Create 4 subsets: X_train, X_test, y_train, y_test\n",
        "* Split the training and testing data roughly 80/20. \n",
        "* To get the same random split every time you run your notebook use `random_state=10`. This helps us get the same results every time and avoid confusion while we're learning. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop('PRICE', axis=1)\n",
        "y = data.PRICE\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "s_mPDbPtCfoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "7Ap2xhoJGFYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "durruvRj-d3-"
      },
      "source": [
        "# Multivariable Regression\n",
        "\n",
        "With 13 features, our Linear Regression model will have the following form:\n",
        "\n",
        "$$ PR \\hat ICE = \\theta _0 + \\theta _1 RM + \\theta _2 NOX + \\theta _3 DIS + \\theta _4 CHAS ... + \\theta _{13} LSTAT$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bO6LDl7yzlw"
      },
      "source": [
        "### Run Your First Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9o1cpLr9dKl"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H1x3WmG9guW"
      },
      "source": [
        "regression = LinearRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6NKCvUK9iEP"
      },
      "source": [
        "regression.fit(X_train, y_train)\n",
        "rsquared = regression.score(X_train, y_train)\n",
        "print(f'R-squared: {rsquared:.3f}')  # R-squared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKc6g5-5dzOt"
      },
      "source": [
        "### Evaluate the Coefficients of the Model\n",
        "\n",
        "Here we do a sense check on our regression coefficients. The first thing to look for is if the coefficients have the expected sign (positive or negative). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUV-Vp1Y9jVA"
      },
      "source": [
        "coefficients = pd.DataFrame(regression.coef_, index=X_train.columns, columns=['Coefficients'])\n",
        "coefficients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the coefficients, an extra room would cost about an additional $3000."
      ],
      "metadata": {
        "id": "a7bZjFVoHLo9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sSyKszdy998"
      },
      "source": [
        "### Analyze the Estimated Values & Regression Residuals\n",
        "\n",
        "The next step is to evaluate the regression. How good the regression is depends not only on the r-squared, it also depends on the **residuals** - the difference between the model's predictions ($\\hat y_i$) and the true values ($y_i$) inside `y_train`. \n",
        "\n",
        "```\n",
        "predicted_values = regr.predict(X_train)\n",
        "residuals = (y_train - predicted_values)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nEqMTRn9owC"
      },
      "source": [
        "predicted_values = regression.predict(X_train)\n",
        "residuals = (y_train - predicted_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.regplot(x=y_train,\n",
        "              y=predicted_values,\n",
        "              scatter_kws={'alpha': 0.4},\n",
        "              line_kws={'color': 'black'})\n",
        "  \n",
        "  plt.xlabel('Actual Prices 1000s')\n",
        "  plt.ylabel('Predicted Prices 1000s')\n",
        "  plt.title('Actual vs Predicted Prices')"
      ],
      "metadata": {
        "id": "Bd2S2RhhUnHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The figure above shows a decent fit with our predicted and the actual values from the dataset."
      ],
      "metadata": {
        "id": "gQpn_z2-DlJ2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlGpvVo29oiH"
      },
      "source": [
        "plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.scatterplot(x=predicted_values,\n",
        "              y=residuals,\n",
        "              alpha=0.8)\n",
        "  \n",
        "  plt.xlabel('Predicted Prices 1000s')\n",
        "  plt.ylabel('Residuals')\n",
        "  plt.title('Predicted Prices vs Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0L9JFSFGmIq"
      },
      "source": [
        "The residuals represent the errors of our model. If there's a pattern in our errors, then our model has a systematic bias.\n",
        "\n",
        "We can analyse the distribution of the residuals. In particular, we're interested in the **skew** and the **mean**.\n",
        "\n",
        "In an ideal case, what we want is something close to a normal distribution. A normal distribution has a skewness of 0 and a mean of 0. A skew of 0 means that the distribution is symmetrical. Here's what a normal distribution looks like: \n",
        "\n",
        "<img src=https://i.imgur.com/7QBqDtO.png height=400>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXmE_Sn49tGl"
      },
      "source": [
        "# plt.figure(figsize=(8,4), dpi=200)\n",
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(x=residuals,\n",
        "              kde=True)\n",
        "  \n",
        "  plt.xlabel('Residuals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resid_skew = residuals.skew()\n",
        "resid_mean = residuals.mean()\n",
        "\n",
        "print(f'Residual Skew: {resid_skew:.3f}')\n",
        "print(f'Residual Mean: {resid_mean:.10f}')"
      ],
      "metadata": {
        "id": "s3FnJGffY7CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC8e42hvdYTd"
      },
      "source": [
        "### Data Transformations for a Better Fit\n",
        "\n",
        "We have two options at this point: \n",
        "\n",
        "1. Change our model entirely. Perhaps a linear model is not appropriate. \n",
        "2. Transform our data to make it fit better with our linear model. \n",
        "\n",
        "Let's try a data transformation approach. \n",
        "\n",
        "First, let's investigate if the target `data['PRICE']` could be a suitable candidate for a log transformation like so:\n",
        "\n",
        "* Use Seaborn's `.displot()` to show a histogram and KDE of the price data. \n",
        "* Calculate the skew of that distribution.\n",
        "* Use [NumPy's `log()` function](https://numpy.org/doc/stable/reference/generated/numpy.log.html) to create a Series that has the log prices\n",
        "* Plot the log prices using Seaborn's `.displot()` and calculate the skew. \n",
        "* Which distribution has a skew that's closer to zero? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4vZt4WP9udK"
      },
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(x=data.PRICE,\n",
        "              kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp7ILTQt9xje"
      },
      "source": [
        "print(f'PRICE Skew: {np.skew(data.PRICE, bias=False):.3f}')\n",
        "print(f'PRICE Mean: {np.mean(data.PRICE):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGrqnjom9xa1"
      },
      "source": [
        "log_price = np.log(data.PRICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with sns.axes_style('whitegrid'):\n",
        "  sns.displot(x=log_price, kde=True)\n",
        "  plt.title(f'Log Prices. Skew is {log_price.skew():.3}')"
      ],
      "metadata": {
        "id": "kUqX8OWCaawU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'log price Skew: {skew(log_price, bias=False):.3f}')"
      ],
      "metadata": {
        "id": "4t8aEUaQadB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK2fGAwsisxg"
      },
      "source": [
        "#### How does the log transformation work?\n",
        "\n",
        "Using a log transformation does not affect every price equally. Large prices are affected more than smaller prices in the dataset. Here's how the prices are \"compressed\" by the log transformation:\n",
        "\n",
        "<img src=https://i.imgur.com/TH8sK1Q.png height=200>\n",
        "\n",
        "We can see this when we plot the actual prices against the (transformed) log prices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LFw0fshk6js"
      },
      "source": [
        "plt.figure(dpi=150)\n",
        "plt.scatter(data.PRICE, np.log(data.PRICE))\n",
        "\n",
        "plt.title('Mapping the Original Price to a Log Price')\n",
        "plt.ylabel('Log Price')\n",
        "plt.xlabel('Actual $ Price in 000s')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6CjqfUD-d4L"
      },
      "source": [
        "## Regression using Log Prices\n",
        "\n",
        "Using log prices instead, our model has changed to:\n",
        "\n",
        "$$ \\log (PR \\hat ICE) = \\theta _0 + \\theta _1 RM + \\theta _2 NOX + \\theta_3 DIS + \\theta _4 CHAS + ... + \\theta _{13} LSTAT $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_491bN0-KnS"
      },
      "source": [
        "X_train, X_test, log_y_train, log_y_test = train_test_split(\n",
        "    X, log_price, test_size=0.2, random_state=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "323lcb3l-LKG"
      },
      "source": [
        "log_regr = LinearRegression()\n",
        "log_regr.fit(X_train, log_y_train)\n",
        "log_rsquared = log_regr.score(X_train, log_y_train)\n",
        "print(f'R-squared for log prices: {log_rsquared:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_predicted_values = log_regr.predict(X_train)\n",
        "log_residuals = log_y_train - log_predicted_values"
      ],
      "metadata": {
        "id": "a9M1DU0ueKk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8xboGOl-d4P"
      },
      "source": [
        "## Evaluating Coefficients with Log Prices\n",
        "\n",
        "* Do the coefficients still have the expected sign? \n",
        "* Is being next to the river a positive based on the data?\n",
        "* How does the quality of the schools affect property prices? What happens to prices as there are more students per teacher? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqavcsE5-NPT"
      },
      "source": [
        "log_coefs = pd.DataFrame(log_regr.coef_, index=X_test.columns, columns=['Coefficients'])\n",
        "log_coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "wbRgqfEt-d4e"
      },
      "source": [
        "## Regression with Log Prices & Residual Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdZ2HKay-PZ4"
      },
      "source": [
        "# Graph of Actual vs. Predicted Log Prices\n",
        "plt.scatter(x=log_y_train, y=log_predicted_values, c='navy', alpha=0.6)\n",
        "plt.plot(log_y_train, log_y_train, color='cyan')\n",
        "plt.title(f'Actual vs Predicted Log Prices: $y _i$ vs $\\hat y_i$ (R-Squared {log_rsquared:.2})', fontsize=17)\n",
        "plt.xlabel('Actual Log Prices $y _i$', fontsize=14)\n",
        "plt.ylabel('Prediced Log Prices $\\hat y _i$', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Original Regression of Actual vs. Predicted Prices\n",
        "plt.scatter(x=y_train, y=predicted_values, c='indigo', alpha=0.6)\n",
        "plt.plot(y_train, y_train, color='cyan')\n",
        "plt.title(f'Original Actual vs Predicted Prices: $y _i$ vs $\\hat y_i$ (R-Squared {rsquared:.3})', fontsize=17)\n",
        "plt.xlabel('Actual prices 000s $y _i$', fontsize=14)\n",
        "plt.ylabel('Prediced prices 000s $\\hat y _i$', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Residuals vs Predicted values (Log prices)\n",
        "plt.scatter(x=log_predicted_values, y=log_residuals, c='navy', alpha=0.6)\n",
        "plt.title('Residuals vs Fitted Values for Log Prices', fontsize=17)\n",
        "plt.xlabel('Predicted Log Prices $\\hat y _i$', fontsize=14)\n",
        "plt.ylabel('Residuals', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Residuals vs Predicted values\n",
        "plt.scatter(x=predicted_values, y=residuals, c='indigo', alpha=0.6)\n",
        "plt.title('Original Residuals vs Fitted Values', fontsize=17)\n",
        "plt.xlabel('Predicted Prices $\\hat y _i$', fontsize=14)\n",
        "plt.ylabel('Residuals', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkU3fgx3tl1w"
      },
      "source": [
        "Now let's calculate the mean and the skew for the residuals using log prices. Are the mean and skew closer to 0 for the regression using log prices?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHPF_CEl-Si2"
      },
      "source": [
        "# Distribution of Residuals (log prices) - checking for normality\n",
        "log_resid_mean = round(log_residuals.mean(), 2)\n",
        "log_resid_skew = round(log_residuals.skew(), 2)\n",
        "\n",
        "sns.displot(log_residuals, kde=True, color='navy')\n",
        "plt.title(f'Log price model: Residuals Skew ({log_resid_skew}) Mean ({log_resid_mean})')\n",
        "plt.show()\n",
        "\n",
        "sns.displot(residuals, kde=True, color='indigo')\n",
        "plt.title(f'Original model: Residuals Skew ({resid_skew}:.3f) Mean ({resid_mean}:.3f)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbvfrJmfmiR"
      },
      "source": [
        "# Compare Out of Sample Performance\n",
        "\n",
        "The *real* test is how our model performs on data that it has not \"seen\" yet. This is where our `X_test` comes in. \n",
        "\n",
        "Our first performance test will, be comparing the r-squared of the two models on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otZnpoaD-VIw"
      },
      "source": [
        "print(f'Original Model Test Data r-squared: {regression.score(X_test, y_test):.2}')\n",
        "print(f'Log Model Test Data r-squared: {log_regr.score(X_test, log_y_test):.2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5Dxrmq41lt"
      },
      "source": [
        "# Predict a Property's Value using the Regression Coefficients\n",
        "\n",
        "Our preferred model now has an equation that looks like this:\n",
        "\n",
        "$$ \\log (PR \\hat ICE) = \\theta _0 + \\theta _1 RM + \\theta _2 NOX + \\theta_3 DIS + \\theta _4 CHAS + ... + \\theta _{13} LSTAT $$\n",
        "\n",
        "The average property has the mean value for all its charactistics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cZzYDIl44Gk"
      },
      "source": [
        "# Starting Point: Average Values in the Dataset\n",
        "features = data.drop(['PRICE'], axis=1)\n",
        "average_vals = features.mean().values\n",
        "property_stats = pd.DataFrame(data=average_vals.reshape(1, len(features.columns)), \n",
        "                              columns=features.columns)\n",
        "property_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRDxU5GrvKz-"
      },
      "source": [
        "Now lets predict how much the average property is worth using the stats above. We will look at the log price estimate and the dollar estimate by reversing the log transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3uzO0LN-hnF"
      },
      "source": [
        "# Make price prediction\n",
        "log_est = log_regr.predict(property_stats)[0]\n",
        "print(f'The log price estimate is ${log_est:.3f}')\n",
        "\n",
        "# Convert log prices to dollar values\n",
        "dollar_est = np.exp(log_est) * 1000\n",
        "print(f'The property is estimated to be worth ${dollar_est:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmy1b_uNc1M7"
      },
      "source": [
        "Keeping the average values for CRIM, RAD, INDUS and others, lets value a property with the following characteristics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfv1eqvmAjML"
      },
      "source": [
        "# Define Property Characteristics\n",
        "next_to_river = True\n",
        "nr_rooms = 8\n",
        "students_per_classroom = 20 \n",
        "distance_to_town = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjMZyxl--joy"
      },
      "source": [
        "# Solution:\n",
        "property_stats['RM'] = nr_rooms\n",
        "property_stats['PTRATIO'] = students_per_classroom\n",
        "property_stats['DIS'] = distance_to_town\n",
        "\n",
        "if next_to_river:\n",
        "    property_stats['CHAS'] = 1\n",
        "else:\n",
        "    property_stats['CHAS'] = 0\n",
        "\n",
        "property_stats['NOX'] = pollution\n",
        "property_stats['LSTAT'] = amount_of_poverty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRYX5n0hgeD_"
      },
      "source": [
        "# Make prediction\n",
        "log_estimate = log_regr.predict(property_stats)[0]\n",
        "print(f'The log price estimate is ${log_estimate:.3}')\n",
        "\n",
        "# Convert Log Prices to Acutal Dollar Values\n",
        "dollar_est = np.e**log_estimate * 1000\n",
        "print(f'The property is estimated to be worth ${dollar_est:.6}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}